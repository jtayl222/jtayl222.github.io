<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-07-24T20:18:09-04:00</updated><id>http://0.0.0.0:4000/feed.xml</id><entry><title type="html">Welcome to Jekyll!</title><link href="http://0.0.0.0:4000/posts/2025-07-02-welcome-to-jekyll" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2025-07-02T19:44:40-04:00</published><updated>2025-07-02T19:44:40-04:00</updated><id>http://0.0.0.0:4000/posts/welcome-to-jekyll</id><content type="html" xml:base="http://0.0.0.0:4000/posts/2025-07-02-welcome-to-jekyll"><![CDATA[<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.]]></summary></entry><entry><title type="html">13 Medium Articles</title><link href="http://0.0.0.0:4000/posts/2025-07-02-13-medium-articles" rel="alternate" type="text/html" title="13 Medium Articles" /><published>2025-07-02T00:00:00-04:00</published><updated>2025-07-02T00:00:00-04:00</updated><id>http://0.0.0.0:4000/posts/13-medium-articles</id><content type="html" xml:base="http://0.0.0.0:4000/posts/2025-07-02-13-medium-articles"><![CDATA[<h1 id="title">title</h1>

<ol>
  <li>Part 4: Tracing a Request Through the Seldon Core v2 MLOps Stack
https://medium.com/@jeftaylo/part-4-tracing-a-request-through-the-seldon-core-v2-mlops-stack-da4a7a3685ae</li>
  <li>Building Production A/B Testing Infrastructure for ML Models
https://medium.com/@jeftaylo/building-production-a-b-testing-infrastructure-for-ml-models-75c8c3b36ba6</li>
  <li>A/B Testing in Production MLOps: Why Traditional Deployments Fail ML Models
https://medium.com/@jeftaylo/a-b-testing-in-production-mlops-why-traditional-deployments-fail-ml-models-bc77bf74c31f</li>
  <li>Enterprise Secret Management in MLOps: Kubernetes Security at Scale
https://medium.com/@jeftaylo/enterprise-secret-management-in-mlops-kubernetes-security-at-scale-a80875e73086</li>
  <li>~7x Speedup: PyTorch on Apple M1 Pro for MLOps
https://medium.com/@jeftaylo/7x-speedup-pytorch-on-apple-m1-pro-for-mlops-fa1c10482c65</li>
  <li>MLOps Engineering Portfolio
https://medium.com/@jeftaylo/mlops-engineering-portfolio-2123456789ab</li>
  <li>Production MLOps with MLflow, Argo, and Kustomize
https://medium.com/@jeftaylo/production-mlops-with-mlflow-argo-and-kustomize-2123456789ac</li>
  <li>Building an MLOps Homelab with Kubernetes
https://medium.com/@jeftaylo/building-an-mlops-homelab-with-kubernetes-2123456789ad</li>
  <li>MLOps Engineering: Production-Ready ML Infrastructure That Scales
https://medium.com/@jeftaylo/mlops-engineering-production-ready-ml-infrastructure-that-scales-2123456789ae</li>
  <li>Automating MLOps: Scalable ML Pipelines
      https://medium.com/@jeftaylo/automating-mlops-scalable-ml-pipelines-2123456789af</li>
  <li>Accelerating MLOps with MLflow
      https://medium.com/@jeftaylo/accelerating-mlops-with-mlflow-2123456789ag</li>
  <li>A Guide to Choosing the Right ML and DL Library
      https://medium.com/@jeftaylo/a-guide-to-choosing-the-right-ml-and-dl-library-2123456789ah</li>
  <li>Adapting Your Messages Array for Any LLM API: A Developer’s Guide
      https://medium.com/@jeftaylo/adapting-your-messages-array-for-any-llm-api-a-developers-guide-2123456789ai</li>
  <li>Using Convolutional Neural Networks for Classification: From Training to Deployment
      https://medium.com/@jeftaylo/using-convolutional-neural-networks-for-classification-from-training-to-deployment-2123456789aj</li>
  <li>Building Intelligent Search &amp; Recommendations: How Vector Databases Supercharge Your NLP Pipelines
      https://medium.com/@jeftaylo/building-intelligent-search-recommendations-how-vector-databases-supercharge-your-nlp-pipelines-2123456789ak</li>
  <li>A Deep Dive into NLP Embeddings: From Word2Vec to text-embedding-ada-002, and Why You Might Want…
      https://medium.com/@jeftaylo/a-deep-dive-into-nlp-embeddings-from-word2vec-to-text-embedding-ada-002-and-why-you-might-want-2123456789al</li>
  <li>Building Interactive Data Apps with FastAPI, Pydantic, and Streamlit: A Real-World Example
      https://medium.com/@jeftaylo/building-interactive-data-apps-with-fastapi-pydantic-and-streamlit-a-real-world-example-2123456789am</li>
  <li>LangGraph’s StateGraph and Conversation History
      https://medium.com/@jeftaylo/langgraphs-stategraph-and-conversation-history-2123456789an</li>
  <li>Why You Shouldn’t Use @tool in LangGraph’s StateGraph Workflows
      https://medium.com/@jeftaylo/why-you-shouldnt-use-tool-in-langgraphs-stategraph-workflows-2123456789ao</li>
  <li>CI/CD Pipelines for Software-Defined Vehicles (SDVs) Incorporating MLflow
      https://medium.com/@jeftaylo/ci-cd-pipelines-for-software-defined-vehicles-sdvs-incorporating-mlflow-2123456789ap</li>
  <li>Building a LangGraph Workflow: Using Tavily Search and GPT-4o for AI-Powered Research
      https://medium.com/@jeftaylo/building-a-langgraph-workflow-using-tavily-search-and-gpt-4o-for-ai-powered-research-2123456789aq</li>
  <li>Kogito Rules (Drools) with Java Inheritance
      https://medium.com/@jeftaylo/kogito-rules-drools-with-java-inheritance-2123456789ar</li>
  <li>Deploying RHPAM on OpenShift 4
      https://medium.com/@jeftaylo/deploying-rhpam-on-openshift-4-2123456789as</li>
  <li>Expose .niogit from Red Hat Business Central
      https://medium.com/@jeftaylo/expose-niogit-from-red-hat-business-central-2123456789at</li>
  <li>Creating maven projects that can be imported into Red Hat Business Central
      https://medium.com/@jeftaylo/creating-maven-projects-that-can-be-imported-into-red-hat-business-central-2123456789au</li>
  <li>BPMN Timers, Red Hat Process Automation Manager, and PostgreSQL 11
      https://medium.com/@jeftaylo/bpmn-timers-red-hat-process-automation-manager-and-postgresql-11-2123456789av</li>
</ol>]]></content><author><name></name></author><summary type="html"><![CDATA[title]]></summary></entry><entry><title type="html">Automating Mlops Scalable Ml Pipelines</title><link href="http://0.0.0.0:4000/posts/2025-07-02-Automating-MLOps-Scalable-ML-Pipelines" rel="alternate" type="text/html" title="Automating Mlops Scalable Ml Pipelines" /><published>2025-07-02T00:00:00-04:00</published><updated>2025-07-02T00:00:00-04:00</updated><id>http://0.0.0.0:4000/posts/Automating-MLOps-Scalable-ML-Pipelines</id><content type="html" xml:base="http://0.0.0.0:4000/posts/2025-07-02-Automating-MLOps-Scalable-ML-Pipelines"><![CDATA[<h1 id="automating-mlops-scalable-ml-pipelines">Automating MLOps: Scalable ML Pipelines</h1>

<p>Automate MLOps pipelines with Ansible, MLflow, and Argo Workflows to build scalable, production-ready ML systems on Kubernetes.</p>

<p>MLOps automation turns notebooks into scalable ML pipelines, bridging data science and production. Using Ansible, MLflow, and Argo Workflows, I built a Kubernetes ML pipeline that streamlines deployment. This automation reflects my MLOps engineering journey, explored in my <a href="https://jeftaylo.medium.com/from-devops-to-mlops-why-employers-care-and-how-i-built-a-fortune-500-stack-in-my-spare-bedroom-ce0d06dd3c61">guide to scalable ML systems</a>.</p>

<hr />

<h2 id="-system-overview-with-automated-stack-setup">🧱 System Overview (with Automated Stack Setup)</h2>

<p><strong>Goal:</strong> Build a real-world MLOps pipeline that starts with <code class="language-plaintext highlighter-rouge">train.py</code> on a laptop and ends with a live model server in Kubernetes.</p>

<p>Key repositories:</p>
<ul>
  <li><a href="https://github.com/jtayl222/ml-platform">ml-platform</a>: Ansible roles and playbooks to install infrastructure and MLOps tools</li>
  <li><a href="https://github.com/jtayl222/homelab-mlops-demo">homelab-mlops-demo</a>: Sample training script, Argo workflows, sealed secrets, and manifests</li>
  <li><a href="https://github.com/jtayl222/aipnd-project">aipnd-project</a>: Model training code that registers models with MLflow and stores artifacts in MinIO</li>
</ul>

<p><strong>Stack Components:</strong></p>
<ul>
  <li>☁️ K3s (lightweight Kubernetes) as the cluster</li>
  <li>💾 MinIO as a local S3 store</li>
  <li>📊 MLflow for experiment tracking and model registry</li>
  <li>🔁 Argo Workflows for pipeline automation</li>
  <li>🔒 Sealed Secrets for secure credential management</li>
</ul>

<p>Ansible automates infrastructure setup for scalable ML pipelines. The <a href="https://github.com/jtayl222/ml-platform">ml-platform</a> repo is organized into <a href="https://github.com/jtayl222/ansible_homelab_kubernetes/tree/main/roles">Ansible roles</a>, each responsible for one component of the MLOps stack. The master playbook, <a href="https://github.com/jtayl222/ansible_homelab_kubernetes/blob/main/playbooks/site.yml"><code class="language-plaintext highlighter-rouge">playbooks/site.yml</code></a>, acts as the automation hub—setting up the cluster, storage, observability, and the full MLOps toolchain.</p>

<p>Each role is idempotent and taggable, so you can run only what you need:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ansible-playbook <span class="nt">-i</span> inventory/production/hosts playbooks/site.yml <span class="nt">--tags</span> mlflow
</code></pre></div></div>

<p>Or launch the full stack in one go. This ensures every service is installed in the right order, and gives you a single, repeatable entry point to spin up your entire homelab environment.</p>

<hr />

<h2 id="-manual-training-step-trainpy">🧪 Manual Training Step: <code class="language-plaintext highlighter-rouge">train.py</code></h2>

<p>Currently, training happens manually (or via Jupyter/CI). Example command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">MLFLOW_TRACKING_URI</span><span class="o">=</span>http://192.168.1.85:32000 <span class="se">\</span>
<span class="nv">MLFLOW_S3_ENDPOINT_URL</span><span class="o">=</span>http://192.168.1.85:30140 <span class="se">\</span>
<span class="nv">AWS_ACCESS_KEY_ID</span><span class="o">=</span>minioadmin <span class="se">\</span>
<span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="o">=</span>minioadmin <span class="se">\</span>
python train.py <span class="nt">--save_dir</span> models <span class="nt">--arch</span> vgg16 <span class="nt">--epochs</span> 5
</code></pre></div></div>

<p>This script:</p>
<ul>
  <li>Logs metrics and params to MLflow</li>
  <li>Registers the model as <code class="language-plaintext highlighter-rouge">cnn_classifier</code></li>
  <li>Saves artifacts to MinIO (<code class="language-plaintext highlighter-rouge">s3://mlflow/</code>)</li>
</ul>

<blockquote>
  <p>💡 These credentials are for local homelab use. In production, use sealed secrets or a vault.</p>
</blockquote>

<p>Once this is done, we’re ready to serve.</p>

<hr />

<h2 id="-role-mlflow_model_server">🚀 Role: <code class="language-plaintext highlighter-rouge">mlflow_model_server</code></h2>

<p>This role picks up only after training is complete. It:</p>
<ul>
  <li>Creates a MinIO bucket (if needed)</li>
  <li>Deploys a Kubernetes Deployment to run <code class="language-plaintext highlighter-rouge">mlflow.models.serve</code></li>
  <li>Waits for the rollout to complete</li>
</ul>

<p>If your model isn’t registered yet, this role fails—which is the correct behavior.</p>

<hr />

<h2 id="-mlops-lesson-order-matters">🧠 MLOps Lesson: Order Matters</h2>

<p>MLOps is about orchestration, not just tools.</p>
<ul>
  <li>Secrets must be in place before workflows can run</li>
  <li>Training must happen before deployment</li>
  <li>Registries must be queried before you try to serve</li>
</ul>

<p>The <code class="language-plaintext highlighter-rouge">mlops_demo_app</code> role is about environment prep. The <code class="language-plaintext highlighter-rouge">mlflow_model_server</code> role is about post-training deployment. They must run in the correct order—and that’s what infrastructure as code allows you to express.</p>

<hr />

<h2 id="-related-work">🔎 Related Work</h2>

<p>For a practical walkthrough of model experimentation and training, see my earlier piece: <a href="https://medium.com/@jeftaylo/using-convolutional-neural-networks-for-classification-from-training-to-deployment-08dd9480dd87">Using Convolutional Neural Networks for Classification — From Training to Deployment</a>.</p>

<p>That article covers:</p>
<ul>
  <li>Using transfer learning and PyTorch to train a classifier</li>
  <li>Tuning hyperparameters and logging metrics with MLflow</li>
  <li>Saving and registering the model to the MLflow model registry</li>
  <li>Model signature, input/output format, and deployment considerations</li>
</ul>

<p>Together, these articles form a complete MLOps narrative—from Jupyter-based training to Kubernetes-based serving.</p>

<hr />

<h2 id="-final-thoughts">📌 Final Thoughts</h2>

<p>You don’t need Terraform. You don’t need AWS. You don’t need to reinvent ML serving.</p>

<p>You just need:</p>
<ul>
  <li>A clear system layout</li>
  <li>Thoughtful sequencing</li>
  <li>Real automation</li>
</ul>

<p>If you’re trying to learn MLOps or build a portfolio, clone <a href="https://github.com/jtayl222">these repos</a> and try to break it. Then try to improve it.</p>

<p>This is what MLOps looks like in practice—not in theory.</p>

<hr />

<p><em>Thanks for reading! For more technical walkthroughs and DevOps/MLOps tooling writeups, visit <a href="https://medium.com/@jeftaylo">medium.com/@jeftaylo</a>.</em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Automating MLOps: Scalable ML Pipelines]]></summary></entry><entry><title type="html">Pytorch M1 Speedup</title><link href="http://0.0.0.0:4000/posts/2025-07-02-pytorch-m1-speedup" rel="alternate" type="text/html" title="Pytorch M1 Speedup" /><published>2025-07-02T00:00:00-04:00</published><updated>2025-07-02T00:00:00-04:00</updated><id>http://0.0.0.0:4000/posts/pytorch-m1-speedup</id><content type="html" xml:base="http://0.0.0.0:4000/posts/2025-07-02-pytorch-m1-speedup"><![CDATA[<h1 id="7x-speedup-pytorch-on-apple-m1-pro-for-mlops">~7x Speedup: PyTorch on Apple M1 Pro for MLOps</h1>

<p><a href="https://jeftaylo.medium.com/?source=post_page---byline--fa1c10482c65---------------------------------------">Jeffrey Taylor</a></p>

<p><em>Achieved ~7x speedup in PyTorch on Apple M1 Pro using MLflow. Learn MLOps optimization &amp; debugging for deep learning. Results vary by setup.</em></p>

<hr />

<h2 id="introduction-the-need-for-speed-in-mlops">Introduction: The Need for Speed in MLOps</h2>

<p>In the fast-paced world of machine learning, MLOps engineers are tasked with delivering faster iterations, quicker insights, and cost-effective solutions.</p>

<p>Initially, I trained a deep learning model for image classification using PyTorch on an Intel Core i7-10710U, achieving an epoch time of 690 seconds. On an Apple M1 Pro, the baseline epoch time was ~563 seconds. While MLflow effectively tracked my experiments, I saw an opportunity for optimization.</p>

<p>This article details how I optimized the training pipeline on the Apple M1 Pro, achieving a <strong>6.8x speedup</strong> (epoch time 82.5 seconds), outperforming the Intel i7 by <strong>8.4x</strong>, all while maintaining robust experiment tracking with MLflow.</p>

<p><em>Your mileage may vary depending on hardware age, model architecture, dataset size, and system configurations like PyTorch or macOS versions.</em></p>

<p><strong>Key Learnings:</strong></p>
<ul>
  <li>Optimize PyTorch workloads for Apple Silicon using Metal Performance Shaders (MPS).</li>
  <li>Debug complex cross-platform issues in MLOps pipelines.</li>
  <li>Leverage MLflow for performance tracking and reproducibility.</li>
  <li>Apply hardware-aware performance engineering in MLOps.</li>
</ul>

<hr />

<h2 id="the-starting-point-performance-on-intel-core-i7-and-apple-m1-pro">The Starting Point: Performance on Intel Core i7 and Apple M1 Pro</h2>

<p><strong>Intel Core i7-10710U:</strong></p>
<ul>
  <li>Epoch Time: 690 seconds.</li>
  <li>Observations: The Python process consumed 550% CPU (5.5 cores out of 12), with overall CPU usage at ~45%.</li>
  <li>Interpretation: MKL optimized CPU computations, but the absence of a dedicated GPU limited parallel computations.</li>
</ul>

<p><strong>Apple M1 Pro (Baseline, CPU-only):</strong></p>
<ul>
  <li>Epoch Time: 563 seconds.</li>
  <li>Interpretation: The M1’s ARM-based CPU was ~18% faster than the Intel i7, likely due to efficiency and unified memory, but still suboptimal without leveraging the GPU.</li>
</ul>

<hr />

<h2 id="hypothesis">Hypothesis</h2>

<p>Both CPUs were limited for deep learning’s parallel computations. I hypothesized that enabling the M1 Pro’s GPU via Metal Performance Shaders (MPS) would unlock significant performance gains, especially compared to the Intel system’s CPU-only setup.</p>

<hr />

<h2 id="the-transition-to-apple-m1-pro-a-new-horizon">The Transition to Apple M1 Pro: A New Horizon</h2>

<p>The Apple M1 Pro is designed for machine learning workloads, with:</p>
<ul>
  <li><strong>ARM-based Architecture:</strong> Optimized for efficiency.</li>
  <li><strong>Unified Memory Architecture:</strong> CPU, GPU, and Neural Engine share high-bandwidth RAM.</li>
  <li><strong>Integrated GPU with Metal Performance Shaders (MPS):</strong> High-performance computing, accessible via PyTorch’s MPS backend.</li>
</ul>

<hr />

<h2 id="the-optimization-journey-debugging-and-discovering-speed">The Optimization Journey: Debugging and Discovering Speed</h2>

<p>To leverage the M1 Pro’s GPU, I updated the device selection logic:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">mps</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> 
    <span class="k">else</span> <span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> 
    <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="challenge-1-runtimeerror-mismatched-tensor-types-in-nnpack-convolutionoutput">Challenge 1: RuntimeError: Mismatched Tensor Types in NNPack ConvolutionOutput</h3>

<p><strong>Problem:</strong><br />
After enabling MPS, encountered:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RuntimeError: Mismatched tensor types in NNPack convolutionOutput
</code></pre></div></div>
<p><strong>Cause:</strong><br />
Tensors (inputs, labels) not explicitly moved to the mps device.</p>

<p><strong>Solution:</strong></p>
<ul>
  <li>Verified MPS availability with <code class="language-plaintext highlighter-rouge">torch.backends.mps.is_available()</code>.</li>
  <li>Ensured all tensors were moved:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>Disabled NNPack with
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">PYTORCH_ENABLE_NNPACK</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">"</span><span class="s">0</span><span class="sh">"</span>
</code></pre></div>    </div>
    <p>as a fallback.</p>
  </li>
</ul>

<h3 id="challenge-2-runtimeerror-too-many-open-files-with-num_workers">Challenge 2: RuntimeError: Too Many Open Files with num_workers</h3>

<p><strong>Problem:</strong><br />
Increasing <code class="language-plaintext highlighter-rouge">num_workers</code> in the DataLoader caused:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RuntimeError: Too many open files
</code></pre></div></div>
<p><strong>Cause:</strong><br />
macOS’s default file descriptor limit (<code class="language-plaintext highlighter-rouge">ulimit -n 256</code>).</p>

<p><strong>Solution:</strong></p>
<ul>
  <li>Temporarily increased the limit:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ulimit</span> <span class="nt">-n</span> 4096
</code></pre></div>    </div>
  </li>
  <li>Set sharing strategy:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
<span class="n">mp</span><span class="p">.</span><span class="nf">set_sharing_strategy</span><span class="p">(</span><span class="sh">'</span><span class="s">file_system</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>Reduced <code class="language-plaintext highlighter-rouge">num_workers</code> and disabled <code class="language-plaintext highlighter-rouge">pin_memory</code>.</li>
</ul>

<h3 id="challenge-3-typeerror-cant-convert-mps-tensor-to-numpy-mlflow-logging">Challenge 3: TypeError: Can’t Convert MPS Tensor to NumPy (MLflow Logging)</h3>

<p><strong>Problem:</strong><br />
Logging an input_example to MLflow raised:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TypeError: can't convert mps:0 device type tensor to numpy
</code></pre></div></div>
<p><strong>Cause:</strong><br />
NumPy requires CPU memory, but MPS tensors are on the GPU.</p>

<p><strong>Solution:</strong></p>
<ul>
  <li>Used <code class="language-plaintext highlighter-rouge">.cpu().detach().numpy()</code> to move tensors to CPU:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">single_input_example_np</span> <span class="o">=</span> <span class="n">single_input_example</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
  <li>Fixed a trailing comma bug.</li>
</ul>

<hr />

<h2 id="the-breakthrough-7x-speedup-on-m1-pro">The Breakthrough: ~7x Speedup on M1 Pro</h2>

<p>After resolving these issues, the M1 Pro achieved an average epoch time of <strong>82.5 seconds</strong>, compared to:</p>
<ul>
  <li>563 seconds (baseline on M1 Pro, CPU-only): ~6.8x speedup.</li>
  <li>690 seconds (Intel i7): ~8.4x speedup.</li>
</ul>

<hr />

<h2 id="why-so-fast">Why So Fast?</h2>

<ul>
  <li><strong>MPS Backend:</strong> Offloads computations to the M1’s GPU, optimized for parallel operations.</li>
  <li><strong>Unified Memory:</strong> Eliminates CPU-GPU data transfer bottlenecks.</li>
  <li><strong>Efficiency:</strong> Leverages specialized hardware for high performance.</li>
</ul>

<p><em>Note: Results may vary based on hardware, model complexity, dataset size, batch size, or software versions.</em></p>

<hr />

<h2 id="mlops-takeaways--conclusion">MLOps Takeaways &amp; Conclusion</h2>

<p>This project demonstrates key MLOps skills:</p>
<ul>
  <li><strong>Performance Engineering:</strong> Achieved ~6.8x speedup on M1 Pro and ~8.4x over Intel i7.</li>
  <li><strong>Hardware Awareness:</strong> Optimized for Apple Silicon’s MPS and unified memory.</li>
  <li><strong>Troubleshooting:</strong> Resolved complex errors across PyTorch and MLflow.</li>
  <li><strong>Cross-Platform Adaptation:</strong> Migrated and optimized a pipeline for a new architecture.</li>
  <li><strong>MLflow Observability:</strong> Tracked experiments for reproducibility.</li>
  <li><strong>Continuous Improvement:</strong> Iteratively refined the pipeline.</li>
</ul>

<p>MLOps engineers must understand compute environments to build scalable solutions.<br />
<em>Explore the code on GitHub, review the MLflow setup, and follow the <a href="https://jeftaylo.medium.com/mlops-engineering-portfolio-3946a461efda">MLOps Engineering Portfolio</a> for more insights.</em></p>

<hr />

<p><a href="https://jeftaylo.medium.com/?source=post_page---post_author_info--fa1c10482c65---------------------------------------">Written by Jeffrey Taylor</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[~7x Speedup: PyTorch on Apple M1 Pro for MLOps]]></summary></entry></feed>